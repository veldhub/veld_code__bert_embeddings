{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47e34076-635a-4867-8279-a0481a844504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ab42e83-e41e-4f6b-ab2f-cb8ea11cd3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['der', 'Kind', 'laufen', 'schnell', '--']\n"
     ]
    }
   ],
   "source": [
    "text = \"Die Kinder liefen schnell.\"\n",
    "doc = nlp(text)\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b73b7926-e3f3-43d6-9487-f7d57932c15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die → Die\n",
      "Kinder → Kinder\n",
      "liefen → liefen\n",
      "schnell → schnell\n",
      ". → .\n"
     ]
    }
   ],
   "source": [
    "# Your predefined list of tokens (e.g., from BERT tokenizer or raw split)\n",
    "tokens = [\"Die\", \"Kinder\", \"liefen\", \"schnell\", \".\"]\n",
    "\n",
    "# Create a Doc\n",
    "doc = Doc(nlp.vocab, words=tokens)\n",
    "\n",
    "# Process it with the pipeline (for POS tagging, lemmatization, etc.)\n",
    "doc = nlp.get_pipe(\"lemmatizer\")(doc)\n",
    "\n",
    "# You can also run full pipeline (optional, slower):\n",
    "# doc = nlp(doc.text)  # or use nlp.pipe([doc]) for many\n",
    "\n",
    "# Inspect lemmas\n",
    "for token in doc:\n",
    "    print(f\"{token.text} → {token.lemma_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73de27bf-8806-42d3-a6ee-2b6d8d9e8321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die → der\n",
      "Kinder → Kind\n",
      "liefen → laufen\n",
      "schnell → schnell\n",
      ". → --\n"
     ]
    }
   ],
   "source": [
    "# Your predefined tokens\n",
    "tokens = [\"Die\", \"Kinder\", \"liefen\", \"schnell\", \".\"]\n",
    "\n",
    "# Create a blank Doc\n",
    "doc = Doc(nlp.vocab, words=tokens)\n",
    "\n",
    "# Run the full spaCy pipeline on the Doc\n",
    "for name, proc in nlp.pipeline:\n",
    "    doc = proc(doc)\n",
    "\n",
    "# Now check lemmas\n",
    "for token in doc:\n",
    "    print(f\"{token.text} → {token.lemma_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cef566b-8965-464b-9216-a0e378d5ec9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7f812d2829e0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7f812d263340>),\n",
       " ('morphologizer',\n",
       "  <spacy.pipeline.morphologizer.Morphologizer at 0x7f812d263580>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7f8130e25690>),\n",
       " ('lemmatizer',\n",
       "  <spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer at 0x7f812d263400>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7f812d023340>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7f8130606810>)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebd2abdf-acab-4a53-a005-6e2f3e948f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die → der\n",
      "Kinder → Kind\n",
      "liefen → laufen\n",
      "schnell → schnell\n",
      ". → --\n"
     ]
    }
   ],
   "source": [
    "# Your predefined list of tokens (e.g., from BERT tokenizer or raw split)\n",
    "tokens = [\"Die\", \"Kinder\", \"liefen\", \"schnell\", \".\"]\n",
    "\n",
    "# Create a Doc\n",
    "doc = Doc(nlp.vocab, words=tokens)\n",
    "\n",
    "# Process it with the pipeline (for POS tagging, lemmatization, etc.)\n",
    "doc = nlp.get_pipe(\"tok2vec\")(doc)\n",
    "#doc = nlp.get_pipe(\"tagger\")(doc)\n",
    "#doc = nlp.get_pipe(\"morphologizer\")(doc)\n",
    "#doc = nlp.get_pipe(\"parser\")(doc)\n",
    "doc = nlp.get_pipe(\"lemmatizer\")(doc)\n",
    "#doc = nlp.get_pipe(\"attribute_ruler\")(doc)\n",
    "#doc = nlp.get_pipe(\"ner\")(doc)\n",
    "\n",
    "# You can also run full pipeline (optional, slower):\n",
    "# doc = nlp(doc.text)  # or use nlp.pipe([doc]) for many\n",
    "\n",
    "# Inspect lemmas\n",
    "for token in doc:\n",
    "    print(f\"{token.text} → {token.lemma_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "343d8ab0-4775-437f-aa66-c577279c34e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die → der\n",
      "Kinder → Kind\n",
      "liefen → laufen\n",
      "schnell → schnell\n",
      ". → --\n"
     ]
    }
   ],
   "source": [
    "# Your predefined list of tokens (e.g., from BERT tokenizer or raw split)\n",
    "tokens = [\"Die\", \"Kinder\", \"liefen\", \"schnell\", \".\"]\n",
    "\n",
    "# Create a Doc\n",
    "doc = Doc(nlp.vocab, words=tokens)\n",
    "\n",
    "for name, proc in nlp.pipeline:\n",
    "    #doc = proc(doc)\n",
    "    doc = nlp.get_pipe(name)(doc)\n",
    "    \n",
    "# You can also run full pipeline (optional, slower):\n",
    "# doc = nlp(doc.text)  # or use nlp.pipe([doc]) for many\n",
    "\n",
    "# Inspect lemmas\n",
    "for token in doc:\n",
    "    print(f\"{token.text} → {token.lemma_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca84d03-19af-4a1b-85aa-5c2e1a08e2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\n",
    "    \"select s.text, e.sentence_id, e.token_index, e.word, e.lemma, e.embedding \"\n",
    "    \"from embeddings__dbmdz__bert_base_german_cased__test as e join sentences as s on e.sentence_id = s.sentence_id \"\n",
    "    \"where lemma='der';\"\n",
    ")\n",
    "rows = cursor.fetchall()\n",
    "colnames = [desc[0] for desc in cursor.description]\n",
    "for row in rows:\n",
    "    row_dict = dict(zip(colnames, row))\n",
    "    print(row_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af197043-272a-4bc7-8b77-cb43ff118438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
